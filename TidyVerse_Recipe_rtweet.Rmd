---
title: "TidyVerse Recipe - rtweet"
author: "humbertohp"
date: "November 27, 2018"
output:
  html_document:
    theme: cerulean
---

## How to get Twitter data with rtweet in R

### Install and load packages (and vignettes for further documentation)

```{r, echo=TRUE}
#devtools::install_github("mkearney/rtweet") # Latest working version of rtweet, this is preferred version to use
#packageVersion("rtweet")

#install.packages("tidyverse")
library(rtweet)
library(tidyverse)
library(knitr)

## quick overview of rtweet functions
## vignette("intro", package = "rtweet")
## working with the stream
## vignette("stream", package = "rtweet")
## troubleshooting
## vignette("FAQ", package = "rtweet")
```

## Twitter API
#### Main Steps:
#### 1) Apply for a Twitter API at: https://developer.twitter.com/en/apply-for-access (A Twitter account is required to go through the process)
####   Fill out the application form by answering the following questions:
####      a) The core use case, intent, or business purpose for your use of the Twitter APIs
####      b) If you intend to analyze Tweets, Twitter users, or their content, share details about the analyses you plan to conduct and the methods or techniques
####      c) If your use involves Tweeting, Retweeting, or liking content, share how you will interact with Twitter users or their content
####      d) If you'll display Twitter content off of Twitter, explain how and where Tweets and Twitter content will be displayed to users of your product or service, including whether Tweets and Twitter content will be displayed at row level or aggregated

#### 2) Create a Twitter App and obtain access tokens
#### See the auth vignette (https://rtweet.info/articles/auth.html) for instructions on obtaining access to Twitter's APIs

#### 3) Create Token in R - Commands commented and keys masked as this step depends on the individual Twitter app created

```{r ,  echo=TRUE}
## appname <- "twitter_analysis"
## key <- "12345678901234567890"
##secret <- "12345678901234567890abcdefghijk"
# create token named "twitter_token"
##twitter_token <- rtweet::create_token(app = appname,
##                                    consumer_key = key,
##                                    consumer_secret = secret)
```

#### Creating the token in R will take you to an authentication step via the browser (interactive). Click on "Authorize the app" button to finalize the process

#### Save twitter_token in your home directory - Commands commented as the token is created interactively and cannot be repro'ed within and RMD file
```{r, echo=TRUE}
# path of home directory 
##home_directory <- "C:/DATA/R Working Dir"
# combine with name for token
##file_name <- file.path(home_directory,
##                       "twitter_token.rds")
# save token to home directory
##saveRDS(twitter_token, file = file_name)

# assuming you followed the procodures to create "file_name"
# from the previous code chunk, then the code below should
# create and save your environment variable.
##cat(paste0("TWITTER_PAT=", file_name),
##    file = file.path(home_directory, ".Renviron"),
##    append = TRUE)
```

## Start collecting and analyzing some Twitter data

```{r, echo=TRUE}
## search for 3000 tweets using the rstats hashtag
rt <- rtweet::search_tweets("#rstats", n = 3000, include_rts = FALSE)
## preview tweets data
rt %>% dplyr::glimpse(10)
## plot time series
ts_plot(rt) +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frequency of #rstats Twitter statuses from past 9 days",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet"
  )
```

### Maps

```{r, echo=TRUE}
## search for 1000 tweets sent from the US
rt <- search_tweets(
  "lang:en", geocode = lookup_coords("usa"), n = 1000
)

## create lat/lng variables using all available tweet and profile geo-location data
rt <- lat_lng(rt)

## plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("state", lwd = .25)

## plot lat and lng points onto state map
with(rt, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```

### Streaming tweets

```{r, echo=TRUE}
## stream tweets mentioning Reinforcement Learning for a week (60 secs)
stream_tweets(
  "Machine Learning",
  timeout = 60,
  file_name = "tweetsaboutml.json",
  parse = FALSE
)

## read in the data as a tidy tbl data frame
mlt <- parse_stream("tweetsaboutml.json")
select(mlt, location, text) %>% kable()
```

### Timelines

```{r, echo=TRUE}
## get the most recent 3000 tweets from cnn, BBCWorld, and foxnews
tmls <- get_timelines(c("cnn", "BBCWorld", "foxnews"), n = 3000)

## plot the frequency of tweets for each user over time
tmls %>%
  dplyr::filter(created_at > "2018-10-01") %>%
  dplyr::group_by(screen_name) %>%
  ts_plot("days", trim = 1L) +
  ggplot2::geom_point() +
  ggplot2::theme_minimal() +
  ggplot2::theme(
    legend.title = ggplot2::element_blank(),
    legend.position = "bottom",
    plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frequency of Tweets posted by news organization",
    subtitle = "Tweet counts aggregated by day from October 2018",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet"
  )
```

### Trends and Favorite Topics

```{r, echo=TRUE}
## Get the 30 most recently favorited tweets by CBS
cbs <- get_favorites("cbs", n = 30)
select(cbs, location, text) %>% head() %>% kable()
## Discover what's currently trending in NYC.
ny <- get_trends("New York")
select(ny, trend, url) %>% head() %>% kable()
```

#### References:
#### http://www.storybench.org/get-twitter-data-rtweet-r/ ; https://rtweet.info/